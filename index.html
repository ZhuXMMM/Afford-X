<!DOCTYPE html>
<html>
<head>
  <meta charset="""utf-8">
  <!-- Meta tags for social media banners -->
  <meta name="""description" content="A Generalizable and Slim Affordance Reasoning Framework for Task-oriented Manipulation">
  <meta property="""og:title" content="""AffordX: A Generalizable and Slim Affordance Reasoning Framework for Task-oriented Manipulation"/>
  <meta property="""og:description" content="""Official implementation of AffordX, a framework for task-oriented manipulation with enhanced generalizability and efficiency."/>
  <meta property="og:url" content="https://mywebsite.com"/>
  <meta property="og:image" content="static/images/teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="AffordX: A Generalizable and Slim Affordance Reasoning Framework for Task-oriented Manipulation">
  <meta name="twitter:description" content="Official implementation of AffordX for task-oriented manipulations with advanced affordance reasoning.">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="affordance reasoning, robotics, generalizability, local deployment, GPT-4V alternative">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>AffordX: Academic Project Page</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">A Generalizable and Slim Affordance Reasoning Framework for Task-oriented Manipulation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#" target="_blank">Xiaomeng Zhu</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Yuyang Li</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Leiyao Cui</a>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Pengfei Li</a>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Huan-ang Gao</a>,
              </span>
              <span class="author-block">
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>
                <i class="fas fa-envelope ml-1"></i>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/view/fromandto" target="_blank">Hao Zhao</a>
                <i class="fas fa-envelope ml-1"></i>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Institute for AI, Peking University<br>Dept of CSE, Hong Kong University of Science and Technology<br>Institute for AI Industry Research, Tsinghua University</span>
              <span class="eql-cntrb"><small style="font-size: 0.8em;"><br><sup>*</sup>Indicates Equal Contribution</small></span>
              <span class="corr-author"><small style="font-size: 0.8em;"><i class="fas fa-envelope ml-1"></i> Corresponding Author </small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2210.10775.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Supplementary</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://github.com/ZhuXMMM/Afford-X" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>

          <span class="link-block">
            <a href="https://arxiv.org/abs/2210.10775" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</div>
</div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Object affordance reasoning, the ability to infer object functionalities based on physical properties, is fundamental for task-oriented planning and activities in both humans and AI. This capability, required for planning and executing daily activities in a task-oriented manner, relies on commonsense knowledge of object physics and functionalities, extending beyond simple object recognition. Current computational models for affordance reasoning from perception lack generalizability, limiting their applicability in novel scenarios. Meanwhile, comprehensive \acp{llm} with emerging reasoning capabilities are challenging to deploy on local devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a large-scale dataset comprising 1,496 tasks and 897k images, designed to enhance the generalizability of affordance reasoning from perception. Utilizing this dataset, we develop Afford-X, an end-to-end trainable affordance reasoning model that incorporates Verb Attention and Bi-Fusion modules to improve multi-modal understanding. This model achieves up to a 25.5\% performance improvement on unseen categories and tasks, while maintaining a compact 187M parameter size and inferring nearly 50 times faster than the GPT-4V API. Our work demonstrates the potential for efficient, generalizable affordance reasoning models that can be deployed on local devices for task-oriented manipulations. We showcase Afford-X's effectiveness in enabling task-oriented manipulations for robots across various tasks and environments, underscoring its efficiency and broad implications for advancing robotics and AI systems in real-world applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Article Teaser Section -->
<section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <!-- Teaser -->
        <img src="static/images/teaser.png" alt="Article Teaser" />
        
        <!-- description -->
        <div class="content has-text-centered">
            <p><strong>Affordance reasoning for task-oriented manipulation.</strong> Afford-X provides efficient visual affordance reasoning through: (a) two comprehensive datasets---COCO-Aff (686k images, 1,144 tasks, 80 categories) and LVIS-Aff (897k images, 1,496 tasks, 1,064 categories); (b) real-time processing (2.38 FPS) with a compact 187M-parameter architecture generating bounding boxes and object masks; (c) robust generalization demonstrated through task-specific object selection and multi-object identification at 0.7 confidence threshold; (d) integration with robotic systems for simulated task-oriented manipulation.</p>
        </div>
      </div>
    </div>
  </section>




<!-- Intro -->
<section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3">Motivation</h2>
        <div class="columns is-centered">
          <div class="column is-eight-fifths">
            <div class="intro content">
              <p>
                In real-world environments, effective robotic interaction transcends mere object identification, emphasizing the understanding of object utilization. This capability, termed <strong>affordance reasoning</strong>, enables robots to infer potential functionalities from physical properties, facilitating diverse tasks from kitchen utensil selection to adaptive tool substitution. However, implementing robust affordance reasoning systems presents two primary challenges: first, the need for high-performance, rapid inference models deployable on robotic development boards, and second, the requirement for a comprehensive knowledge base capable of handling diverse scenarios.
              </p>
              <!-- <p>
                To address these challenges, we propose a slim end-to-end multimodal framework—<strong><em>Afford-X</em></strong>—that distills knowledge from a label-informed teacher model to a pronoun-only student model (e.g., “something”). Additionally, we construct large-scale “task–object” pairs using an automated pipeline that augments traditional object detection datasets with minimal manual labeling, thereby expanding the affordance knowledge base.
              </p> -->
              <p><strong>Key Contributions:</strong></p>
              <ul>
                <li><em>Novel Framework:</em> We introduce <strong>Afford-X</strong>, a knowledge distillation-based slim and efficient model that achieves real-time inference for task-oriented manipulation.</li>
                <li><em>Advanced Modules:</em> Verb Attention and Bi-Fusion jointly enhance action-focus and multimodal integration.</li>
                <li><em>Large-Scale Datasets:</em> Our automated pipeline generates extensive affordance reasoning corpora (COCO-Aff and LVIS-Aff) via GPT-4-based processing.</li>
                <li><em>Empirical Validation:</em> Experiments in simulated and real-world scenarios show that our approach outperforms conventional detection-based or heavyweight generative models across diverse dynamic tasks.</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>


<!-- Approach Section -->
<section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-3">Approach</h2>
        
        <!-- First Image -->
        <div class="content">
          <img src="static/images/framework.png" alt="Framework" style="max-width: 100%; height: auto; display: block; margin: 0 auto;" />
        </div>
        
        <!-- First Description -->
        <div class="content">
          <p style="text-align: left;">
            Our Afford-X pipeline processes visual features from images and textual features from prompts through two key components: a <strong>Bi-Fusion (BF)</strong> module that aligns text-visual representations via cross-attention, and a <strong>Verb Attention (VA)</strong> module that emphasizes action-related cues. The enhanced features feed into a transformer encoder-decoder, producing parallel outputs for object detection and segmentation, enabling task-specific object identification even without explicit object labels.
          </p>
        </div>
        
        <!-- Second Image -->
        <div class="content">
          <img src="static/images/method_pipeline.png" alt="Method Pipeline" style="max-width: 100%; height: auto; display: block; margin: 0 auto;" />
        </div>
        
        <!-- Second Description -->
        <div class="content">
          <p style="text-align: left;">
            Our noun-pronoun distillation framework comprises teacher and student encoder-decoders. The teacher learns from explicit object labels (e.g., "couch"), storing noun features in a memory bank, while the student processes pronoun inputs (e.g., "something") guided by these stored prototypes. A soft binary target loss aligns their bounding box predictions, enabling category-agnostic affordance detection while preserving discriminative power.
          </p>
        </div>
        
      </div>
    </div>
  </section>





<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{affordx2025,
  title={A Generalizable and Slim Affordance Reasoning Framework for Task-oriented Manipulation},
  author={...},
  journal={...},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
