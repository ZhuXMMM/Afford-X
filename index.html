<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners -->
  <meta name="description" content="AffordX: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation">
  <meta property="og:title" content="AffordX: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation"/>
  <meta property="og:description" content="Official implementation of AffordX, a framework for task-oriented manipulation with enhanced generalizability and efficiency."/>
  <meta property="og:url" content="https://mywebsite.com"/>
  <meta property="og:image" content="static/images/teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="AffordX: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation">
  <meta name="twitter:description" content="Official implementation of AffordX for task-oriented manipulations with advanced affordance reasoning.">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="affordance reasoning, robotics, generalizability, local deployment, GPT-4V alternative">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>AffordX: Academic Project Page</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script>
    // 添加视频加载错误处理
    function handleVideoError(video) {
      console.error('Video loading error:', video.error);
    }
    
    // 添加视频加载状态监控
    function handleVideoLoaded(video) {
      console.log('Video loaded successfully');
    }
    
    window.onload = function() {
      const video = document.getElementById('tree');
      video.onerror = () => handleVideoError(video);
      video.onloadeddata = () => handleVideoLoaded(video);
    }
    </script>
  <!-- 自定义高亮样式 -->
  <style>
    /* 1) 卡片高亮：用 box-shadow、圆角使其突出，而不改变原背景与文字色。 */
    .highlighted-card {
      position: relative;
      z-index: 10; /* 让其层级更高 */
      border-radius: 6px; /* 与 box 的默认圆角一致或自定义 */
      box-shadow: 0 0 0 4px #209cee, 0 6px 16px rgba(0, 0, 0, 0.2);
      transition: box-shadow 0.3s ease;
    }
    /* 鼠标悬浮时可再加一层阴影(可选) */
    .highlighted-card:hover {
      box-shadow: 0 0 0 4px #209cee, 0 8px 20px rgba(0, 0, 0, 0.25);
    }

    /* 2) 下方gif的外围容器高亮：与卡片相同的 box-shadow、圆角、再加点padding使之更大一点 */
    .highlighted-gif-wrapper {
      display: inline-block;
      position: relative;
      z-index: 10;
      border-radius: 6px;
      box-shadow: 0 0 0 4px #209cee, 0 6px 16px rgba(0, 0, 0, 0.2);
      padding: 0.5rem; /* 让外框比图片稍大 */
      transition: box-shadow 0.3s ease;
    }

    /* .highlighted-gifWrapper 在 hovered 状态下（可选） */
    .highlighted-gif-wrapper:hover {
      box-shadow: 0 0 0 4px #209cee, 0 8px 20px rgba(0, 0, 0, 0.25);
    }

    /* 3) 图片默认宽度为 100%，JS里会根据需求再覆盖 style.width = '50%'/'66%'/'88%' */
    #dataset-gif {
      max-width: 100%;
      height: auto;
      border-radius: 4px; 
    }

    .video-wrapper video {
  width: 100%;
} 
  </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AffordX: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#" target="_blank">Xiaomeng Zhu</a><sup>†</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Yuyang Li</a><sup>†</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Leiyao Cui</a>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Pengfei Li</a>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Huan-ang Gao</a>,
              </span>
              <span class="author-block">
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a><sup><i class="fas fa-envelope ml-1"></i></sup>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/view/fromandto" target="_blank">Hao Zhao</a><sup><i class="fas fa-envelope ml-1"></i></sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Institute for AI, Peking University<br>Dept of CSE, Hong Kong University of Science and Technology<br>Institute for AI Industry Research, Tsinghua University</span>
              <span class="eql-cntrb"><small style="font-size: 0.8em;"><br><sup>†</sup>Indicates Equal Contribution</small></span>
              <span class="corr-author"><small style="font-size: 0.7em;"><sup><i class="fas fa-envelope ml-1"></i></sup> Corresponding Author </small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2210.10775.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Supplementary</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://github.com/ZhuXMMM/Afford-X" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>

          <span class="link-block">
            <a href="https://arxiv.org/abs/2210.10775" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</div>
</div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="padding:0 0 0 0;position:relative;">
        <iframe weight=200% src="https://player.vimeo.com/video/1059018352?h=e8e39bbe6b" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>
      </div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Object affordance reasoning, the ability to infer object functionalities based on physical properties, is fundamental for task-oriented planning and activities in both humans and AI. This capability, required for planning and executing daily activities in a task-oriented manner, relies on commonsense knowledge of object physics and functionalities, extending beyond simple object recognition. Current computational models for affordance reasoning from perception lack generalizability, limiting their applicability in novel scenarios. Meanwhile, comprehensive \acp{llm} with emerging reasoning capabilities are challenging to deploy on local devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a large-scale dataset comprising 1,496 tasks and 897k images, designed to enhance the generalizability of affordance reasoning from perception. Utilizing this dataset, we develop Afford-X, an end-to-end trainable affordance reasoning model that incorporates Verb Attention and Bi-Fusion modules to improve multi-modal understanding. This model achieves up to a 25.5\% performance improvement on unseen categories and tasks, while maintaining a compact 187M parameter size and inferring nearly 50 times faster than the GPT-4V API. Our work demonstrates the potential for efficient, generalizable affordance reasoning models that can be deployed on local devices for task-oriented manipulations. We showcase Afford-X's effectiveness in enabling task-oriented manipulations for robots across various tasks and environments, underscoring its efficiency and broad implications for advancing robotics and AI systems in real-world applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Article Teaser Section -->
<section class="hero is-small is-info">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <!-- 上方放置图片 -->
        <div class="columns is-centered">
          <div class="column is-12">
            <figure class="image is-fullwidth box">
              <img src="static/images/teaser.png" alt="Article Teaser" style="width:100%; height:auto;">
            </figure>
          </div>
        </div>
        <!-- 下方放置文字 -->
        <div class="columns is-centered">
          <div class="column is-12 content has-text-centered">
            <p class="title is-4 has-text-white">
              <strong>Affordance reasoning for task-oriented manipulation</strong>
            </p>
            <p class="subtitle is-6 has-text-light" style="text-align: left;">
              Afford-X provides efficient visual affordance reasoning through:
              (a) two comprehensive datasets—COCO-Aff (686k images, 1,144 tasks, 80 categories) 
              and LVIS-Aff (897k images, 1,496 tasks, 1,064 categories);
              (b) real-time processing (2.38 FPS) with a compact 187M-parameter architecture generating 
              bounding boxes and object masks;
              (c) robust generalization demonstrated through task-specific object selection and 
              multi-object identification at a 0.7 confidence threshold;
              (d) integration with robotic systems for simulated task-oriented manipulation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


<!-- Dataset Statistics & Visualization -->
<section class="section hero is-medium is-light" style="padding-top: 1rem; padding-bottom: 1rem;">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered mb-6">Dataset Statistics & Visualization</h2>
      <div class="columns is-centered is-variable is-6 is-multiline">
        <!-- COCO-Tasks -->
        <div class="column is-4 has-text-centered" id="coco-tasks-card">
          <div class="box">
            <h3 class="title is-4">COCO-Tasks</h3>
            <p class="subtitle is-6 mt-4">
              <img src="static/images/icon/gallery.png" alt="Images Icon" style="width:1em; vertical-align: middle; margin-right: 0.5em;">
              <strong>40,000 Images</strong>
            </p>
            <p class="subtitle is-6 mt-2">
              <img src="static/images/icon/check.png" alt="Tasks Icon" style="width:1em; vertical-align: middle; margin-right: 0.5em;">
              <strong>14 Tasks</strong>
            </p>
            <p class="subtitle is-6 mt-2">
              <img src="static/images/icon/categories.png" alt="Categories Icon" style="width:1em; vertical-align: middle; margin-right: 0.5em;">
              <strong>&lt; 80 Categories</strong>
            </p>
          </div>
        </div>

        <!-- COCO-Aff -->
        <div class="column is-4 has-text-centered" id="coco-aff-card">
          <div class="box">
            <h3 class="title is-4">COCO-Aff</h3>
            <p class="subtitle is-6 mt-4">
              <img src="static/images/icon/gallery.png" alt="Images Icon" style="width:1em; vertical-align: middle; margin-right: 0.5em;">
              <strong>686,000 Images</strong>
            </p>
            <p class="subtitle is-6 mt-2">
              <img src="static/images/icon/check.png" alt="Tasks Icon" style="width:1em; vertical-align: middle; margin-right: 0.5em;">
              <strong>1,144 Tasks</strong>
            </p>
            <p class="subtitle is-6 mt-2">
              <img src="static/images/icon/categories.png" alt="Categories Icon" style="width:1em; vertical-align: middle; margin-right: 0.5em;">
              <strong>80 Categories</strong>
            </p>
          </div>
        </div>

        <!-- LVIS-Aff -->
        <div class="column is-4 has-text-centered" id="lvis-aff-card">
          <div class="box">
            <h3 class="title is-4">LVIS-Aff</h3>
            <p class="subtitle is-6 mt-4">
              <img src="static/images/icon/gallery.png" alt="Images Icon" style="width:1em; vertical-align: middle; margin-right: 0.5em;">
              <strong>897,000 Images</strong>
            </p>
            <p class="subtitle is-6 mt-2">
              <img src="static/images/icon/check.png" alt="Tasks Icon" style="width:1em; vertical-align: middle; margin-right: 0.5em;">
              <strong>1,496 Tasks</strong>
            </p>
            <p class="subtitle is-6 mt-2">
              <img src="static/images/icon/category.png" alt="Categories Icon" style="width:1em; vertical-align: middle; margin-right: 0.5em;">
              <strong>1,064 Categories</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- 用于显示对应gif的外围高亮容器 -->
      <div class="columns is-centered" style="margin-top: 2rem;">
        <div class="column is-10 has-text-centered">
          <div id="gif-wrapper" class="">
            <img id="dataset-gif" src="" alt="Dataset Visualization GIF">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Dataset Statistics & Visualization -->


<!-- Task-oriented Manipulation in Diverse Environments -->
<section class="hero is-medium" style="padding-top: 2rem; padding-bottom: 2rem;">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered mb-6">Task-oriented Manipulation in Diverse Environments</h2>

      <!-- 五个任务的标题行（可选） -->
      <div class="columns is-mobile has-text-centered" style="font-weight:600; margin-bottom:1rem;">
        <div class="column">clean electronic screens with</div>
        <div class="column">drink water with</div>
        <div class="column">protect items from rain with</div>
        <div class="column">spread butter with</div>
        <div class="column">steam video with</div>
      </div>

      <!-- 6 行，每行一个 scene，5 列（1~5 对应以上五个任务） -->
      <!-- scene1 -->
      <div class="columns is-mobile">
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene1/1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene1/2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene1/3.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene1/4.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene1/5.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <!-- scene2 -->
      <div class="columns is-mobile">
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene2/1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene2/2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene2/3.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene2/4.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene2/5.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <!-- scene3 -->
      <div class="columns is-mobile">
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene3/1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene3/2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene3/3.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene3/4.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene3/5.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <!-- scene4 -->
      <div class="columns is-mobile">
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene4/1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene4/2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene4/3.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene4/4.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene4/5.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <!-- scene5 -->
      <div class="columns is-mobile">
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene5/1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene5/2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene5/3.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene5/4.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene5/5.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <!-- scene6 -->
      <div class="columns is-mobile">
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene6/1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene6/2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene6/3.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene6/4.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video autoplay muted style="width:100%;" >
            <source src="static/videos/classify/scene6/5.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>
  </div>
</section>
<!-- End Task-oriented Manipulation in Diverse Environments -->
 
<!-- Intro -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Key Contributions</h2>
      <div class="columns is-centered is-multiline">
        <div class="column is-8">
          <div class="box">
            <div class="content">
              <ul>
                <li><em>Novel Framework:</em> We introduce <strong>Afford-X</strong>, a knowledge distillation-based slim
                  and efficient model that achieves real-time inference for task-oriented manipulation.</li>
                <li><em>Advanced Modules:</em> Verb Attention and Bi-Fusion jointly enhance action-focus and
                  multimodal integration.</li>
                <li><em>Large-Scale Datasets:</em> Our automated pipeline generates extensive affordance reasoning
                  corpora (COCO-Aff and LVIS-Aff) via GPT-4-based processing.</li>
                <li><em>Empirical Validation:</em> Experiments in simulated and real-world scenarios show that our
                  approach outperforms conventional detection-based or heavyweight generative models across diverse
                  dynamic tasks.</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Intro -->

<!-- Approach Section -->
<section class="section hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Framework</h2>

      <!-- First row: BF + VA modules, now vertical -->
      <div class="columns is-variable is-7 is-centered">
          <!-- Image (full-width) -->
          <div class="column is-10">
          <figure class="image is-fullwidth box">
              <img src="static/images/method_pipeline.png" alt="Framework" style="width:100%; height:auto;">
          </figure>
          </div>
      </div>
      
      <div class="columns is-variable is-7 is-centered">
          <!-- Text (full-width) -->
          <div class="column is-10">
          <div class="content" style="text-align: left;">
              <p>
              Our Afford-X pipeline processes visual features from images and textual features from prompts
              through two key components: a <strong>Bi-Fusion (BF)</strong> module that aligns text-visual
              representations via cross-attention, and a <strong>Verb Attention (VA)</strong> module that
              emphasizes action-related cues. The enhanced features feed into a transformer encoder-decoder,
              producing parallel outputs for object detection and segmentation, enabling task-specific object
              identification even without explicit object labels.
              </p>
          </div>
          </div>
      </div>

      <hr class="dropdown-divider" />

      <!-- Second row: Distillation Framework -->
      <div class="columns is-centered">
          <!-- Image (full-width) -->
          <div class="column is-6">
            <figure class="image is-fullwidth box">
              <img src="static/images/framework.png" alt="Method Pipeline" style="width:100%; height:auto;">
            </figure>
          </div>
        </div>
        
        <div class="columns is-centered">
          <!-- Text (full-width) -->
          <div class="column is-10">
            <div class="content" style="text-align: left;">
              <p>
                Our noun-pronoun distillation framework comprises teacher and student encoder-decoders. The
                teacher learns from explicit object labels (e.g., "couch"), storing noun features in a memory
                bank, while the student processes pronoun inputs (e.g., "something") guided by these stored
                prototypes. A soft binary target loss aligns their bounding box predictions, enabling
                category-agnostic affordance detection while preserving discriminative power.
              </p>
            </div>
          </div>
        </div>

    </div>
  </div>
</section>
<!-- End Approach Section -->


<!-- Dataset Construction Section -->
<section class="section hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Dataset Construction</h2>

      <!-- Row for Image -->
      <div class="columns is-centered">
        <div class="column is-10">
          <figure class="image is-fullwidth box">
            <img src="static/images/dataset_construction.png" alt="Dataset Construction" style="width:100%; height:auto;">
          </figure>
        </div>
      </div>

      <!-- Row for Text -->
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="content" style="text-align: left;">
            <p>
              Our automated pipeline converts conventional object detection annotations into large-scale
              affordance datasets with minimal human intervention. The process starts by generating a set of
              diverse task prompts for each object category (step 1), then matches these prompts to relevant
              categories using commonsense preference rankings (step 2). Next, a quality inspection step
              filters out inappropriate or duplicative pairs (step 3). Finally, image sampling is conducted
              to ensure balanced coverage of different categories and layouts (step 4), producing
              comprehensive affordance-task pairs.
            </p>
          </div>
        </div>
      </div>
      
    </div>
  </div>
</section>
<!-- End Dataset Construction Section -->


<!-- EMBODIED AFFORDANCE REASONING -->
<section class="section hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Embodied Affordance Reasoning</h2>
      
      <!-- 上方放置图片 -->
      <div class="columns is-centered">
        <div class="column is-10">
          <figure class="image is-fullwidth box">
            <img src="static/images/infra_compressed.png"
                 alt="Embodied Affordance Reasoning"
                 style="width:100%; height:auto;">
          </figure>
        </div>
      </div>
      
      <!-- 下方放置文字 -->
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="content" style="text-align: left;">
            <p>
              To deploy Afford-X in real-world or simulated robotics environments, we adopt a system
              architecture that begins with RGB image processing for object masks (a). Optionally, the robot
              may build a geometric map (b) for robust path planning. Based on these perceptual outputs, the
              robot positions itself to optimize viewpoint and executes the planned manipulation (c). For
              multiple sub-tasks (e.g., "build up a space for working"), the system recursively applies the
              affordance-driven selection and action cycle (d).
            </p>
          </div>
        </div>
      </div>
      
    </div>
  </div>
</section>
<!-- End EMBODIED AFFORDANCE REASONING -->


<!-- Key Experimental Results Section -->
<section class="section hero is-light is-small" id="key-experiments">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Key Experimental Results</h2>
      <div class="content box" style="margin-top: 2em;">

        <!-- (1) SOTA Comparison Table -->
        <p class="subtitle is-5"><strong>State-of-the-Art Comparison</strong></p>
        <p>
          Below, we compare <em>Afford-X</em> against existing methods on 
          <em>COCO-Tasks</em>, 
          <span style="color:blue"><strong>COCO-Aff</strong></span> (686k images, 1,144 tasks),
          and 
          <span style="color:orange"><strong>LVIS-Aff</strong></span> (897k images, 1,496 tasks). 
          Our model consistently achieves superior performance in both affordance understanding (mAP<sub>box</sub>) 
          and instance segmentation (mAP<sub>mask</sub>). <br/>
          Bold values indicate the best performance, while underlined values indicate the second-best.
        </p>

        <div class="table-container" style="margin-bottom: 1.5em;">
          <table class="table is-fullwidth is-bordered is-striped is-hoverable">
            <caption style="caption-side: top; font-weight: 600;">
              <em>
                Comparison of Afford-X with state-of-the-art methods.
              </em>
            </caption>
            <thead>
              <tr>
                <th rowspan="2"><strong>Index</strong></th>
                <th rowspan="2"><strong>Method</strong></th>
                <th colspan="2"><strong>COCO-Tasks</strong></th>
                <th colspan="2"><span style="color:blue;"><strong>COCO-Aff</strong></span></th>
                <th colspan="2"><span style="color:orange;"><strong>LVIS-Aff</strong></span></th>
              </tr>
              <tr>
                <th>mAP<sub>box</sub></th>
                <th>mAP<sub>mask</sub></th>
                <th>mAP<sub>box</sub></th>
                <th>mAP<sub>mask</sub></th>
                <th>mAP<sub>box</sub></th>
                <th>mAP<sub>mask</sub></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>(a)</td>
                <td>Fast R-CNN + GGNN<sup>†</sup></td>
                <td>32.6</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
              </tr>
              <tr>
                <td>(b)</td>
                <td>YOLO + GGNN<sup>†</sup></td>
                <td>33.2</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
              </tr>
              <tr>
                <td>(c)</td>
                <td>MDETR (w/o pretraining) + GGNN<sup>†</sup></td>
                <td>9.6</td>
                <td>8.6</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
              </tr>
              <tr>
                <td>(d)</td>
                <td>MDETR + GGNN<sup>†</sup></td>
                <td>36.8</td>
                <td>30.3</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
              </tr>
              <!-- Keep only VitDet (ViT-H) row -->
              <tr>
                <td>(g)</td>
                <td>ViTDet (ViT-H) + GGNN</td>
                <td>33.8</td>
                <td>25.9</td>
                <td>31.5</td>
                <td>26.1</td>
                <td>8.5</td>
                <td>7.4</td>
              </tr>
              <tr>
                <td>(h)</td>
                <td>MDETR<sup>†</sup></td>
                <td>41.3</td>
                <td>35.2</td>
                <td>44.7</td>
                <td>41.0</td>
                <td>25.1</td>
                <td>22.7</td>
              </tr>
              <tr>
                <td>(i)</td>
                <td>MDETR (w/ VA &amp; BF)</td>
                <td>43.2</td>
                <td>36.9</td>
                <td><u>45.2</u></td>
                <td><u>41.4</u></td>
                <td><u>26.8</u></td>
                <td><u>24.2</u></td>
              </tr>
              <tr>
                <td>(j)</td>
                <td>TOIST<sup>†</sup></td>
                <td><u>44.1</u></td>
                <td><u>39.0</u></td>
                <td>44.9</td>
                <td>41.3</td>
                <td>26.2</td>
                <td>23.4</td>
              </tr>
              <tr>
                <td>(k)</td>
                <td><strong>Afford-X (w/ VA &amp; BF)</strong></td>
                <td><strong>45.3</strong></td>
                <td><strong>39.2</strong></td>
                <td><strong>45.8</strong></td>
                <td><strong>42.5</strong></td>
                <td><strong>27.7</strong></td>
                <td><strong>24.8</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p class="is-size-6 has-text-grey">
          <em>
            † Results from original papers. <br/>
            Afford-X consistently outperforms baselines across all datasets, demonstrating robust affordance reasoning.
          </em>
        </p>

        <hr style="margin-top:2em; margin-bottom:2em;">

        <!-- (2) Efficiency Comparison Table -->
        <p class="subtitle is-5"><strong>Computational Efficiency</strong></p>
        <p>
          We also compare runtime efficiency, showing frames per second (FPS) and parameter counts 
          for various \ac{llm}-based pipelines versus our approach on an NVIDIA 3090 GPU:
        </p>
        <div class="table-container" style="margin-bottom: 1.5em;">
          <table class="table is-fullwidth is-bordered is-striped is-hoverable">
            <caption style="caption-side: top; font-weight: 600;">
              <em>Computational efficiency comparison</em>
            </caption>
            <thead>
              <tr>
                <th><strong>Index</strong></th>
                <th><strong>Method</strong></th>
                <th><strong>FPS</strong></th>
                <th><strong>Parameters</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>(a)</td>
                <td>Detection + GPT-4</td>
                <td>1.18</td>
                <td>&gt;369M</td>
              </tr>
              <tr>
                <td>(b)</td>
                <td>Detection + BLIP + GPT-4</td>
                <td>0.27</td>
                <td>&gt;498M</td>
              </tr>
              <tr>
                <td>(c)</td>
                <td>GPT-4 + OpenSeeD</td>
                <td>0.11</td>
                <td>&gt;116M</td>
              </tr>
              <tr>
                <td>(d)</td>
                <td>GPT-4V + OpenSeeD</td>
                <td>0.04</td>
                <td>&gt;116M</td>
              </tr>
              <tr>
                <td>(e)</td>
                <td>SPHINX + OpenSeeD</td>
                <td>0.11</td>
                <td>1.2B</td>
              </tr>
              <tr>
                <td>(f)</td>
                <td>SPHINX (CoT)</td>
                <td>0.49</td>
                <td>1.1B</td>
              </tr>
              <tr>
                <td>(g)</td>
                <td>COCO-Aff (Ours)</td>
                <td><strong>2.38</strong></td>
                <td><strong>187M</strong></td>
              </tr>
              <tr>
                <td>(h)</td>
                <td>LVIS (Ours)</td>
                <td><strong>2.38</strong></td>
                <td><strong>187M</strong></td>
              </tr>
            </tbody>
          </table>
        </div>

        <hr style="margin-top:2em; margin-bottom:2em;">

        <!-- (3) Precision Comparison Figure -->
        <p class="subtitle is-5"><strong>Precision Comparison</strong></p>
        <div class="box" style="text-align:center;">
          <figure class="image is-fullwidth">
            <img src="static/images/combined_metrics_a.png" alt="Precision comparison of different methods"
                 style="width:100%; height:auto;">
          </figure>
          <p class="is-size-6 has-text-grey" style="margin-top:0.75em;">
            <em>Violin plot comparing precision of LLM-based methods vs. Afford. 
              Stars denote statistical significance. Afford-X achieves notably higher precision 
              with fewer parameters and faster runtime.</em>
          </p>
        </div>

        <hr style="margin-top:2em; margin-bottom:2em;">

        <!-- (Optional) Additional Summary -->

      </div>
    </div>
  </div>
</section>


<!-- BibTex citation (unchanged) -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{affordx2025,
    title={A Generalizable and Slim Affordance Reasoning Framework for Task-oriented Manipulation},
    author={...},
    journal={...},
    year={2025}
  }</code></pre>
  </div>
</section>
<!-- End BibTex citation -->


<!-- JS脚本放在页面底部，控制数据集卡片与gif的轮播 -->
<script>
  // 数据集配置：加上 scale 字段以控制三张图的相对宽度
  var datasets = [
    {
      name: "COCO-Tasks",
      cardId: "coco-tasks-card",
      gifSrc: "static/images/output_gifs/cocotasks/result.png",
      scale: "60%"
    },
    {
      name: "COCO-Aff",
      cardId: "coco-aff-card",
      gifSrc: "static/images/output_gifs/cocoaff/result.png",
      scale: "80%"
    },
    {
      name: "LVIS-Aff",
      cardId: "lvis-aff-card",
      gifSrc: "static/images/output_gifs/lvisaff/result.png",
      scale: "100%"
    }
  ];

  var currentIndex = 0;

  function highlightDataset(index) {
    // 先移除所有卡片的高亮
    datasets.forEach(function(dataset){
      document.getElementById(dataset.cardId).classList.remove("highlighted-card");
    });
    // 为当前数据集卡片添加高亮
    var selected = datasets[index];
    document.getElementById(selected.cardId).classList.add("highlighted-card");

    // 更新下方gif
    var gifElem = document.getElementById("dataset-gif");
    gifElem.src = selected.gifSrc;
    gifElem.style.width = selected.scale; // 让图显示为对应的百分比

    // 高亮下方容器
    var gifWrapper = document.getElementById("gif-wrapper");
    // 可以先移除后再添加，保证在每次切换时重置动画
    gifWrapper.classList.remove("highlighted-gif-wrapper");
    // 这里延时 1ms 再添加，可在切换时有闪动效果，也可直接添加
    setTimeout(function(){
      gifWrapper.classList.add("highlighted-gif-wrapper");
    }, 1);
  }

  function nextDataset() {
    currentIndex = (currentIndex + 1) % datasets.length;
    highlightDataset(currentIndex);
  }

  // 页面加载完后自动开始轮播
  window.addEventListener("load", function(){
    highlightDataset(currentIndex);
    // 每隔4秒切换下一个数据集
    setInterval(nextDataset, 4000);
  });


  function handleVideoEnd(video) {
  // 保持在最后一帧
  video.currentTime = video.duration;
  
  // 15秒后重新播放
  setTimeout(() => {
    video.currentTime = 0;
    video.play();
  }, 15000);
}
</script>

</body>
</html>